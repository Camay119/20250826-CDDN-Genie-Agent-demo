{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7723253f-17b8-423a-9528-1244bdb959fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3. GenieAgent を用いたマルチエージェントシステム\n",
    "\n",
    "## 概要\n",
    "- **databricks_langchain.genie.GenieAgent** を使用\n",
    "- 2つのGenie Spaceを、**エージェントノード**として定義する\n",
    "- **Supervisor Agent** によるルーティングを行う\n",
    "- **マルチエージェントアーキテクチャ** の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "880e84fa-62cb-454d-afa7-9a59aa596a58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 必要なパッケージのインストール\n",
    "%pip install -U -qqq mlflow-skinny[databricks] langgraph==0.3.4 databricks-langchain databricks-agents uv\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13c87b8e-90fa-4a5e-9a5e-4e1f63a2471c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Agent.py の作成（Models From Code）\n",
    "\n",
    "%%writefile を使用してagent.pyファイルを作成します。これはMLflow Models From Codeで使用されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "197452b6-6872-43cb-af45-620db755142e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile agent.py\n",
    "import functools\n",
    "import os\n",
    "from typing import Any, Generator, Literal, Optional\n",
    "\n",
    "import mlflow\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks_langchain import ChatDatabricks\n",
    "from databricks_langchain.genie import GenieAgent\n",
    "from langchain_core.runnables import RunnableLambda, RunnableConfig\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from mlflow.langchain.chat_agent_langgraph import ChatAgentState\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import (\n",
    "    ChatAgentChunk,\n",
    "    ChatAgentMessage,\n",
    "    ChatAgentResponse,\n",
    "    ChatContext,\n",
    ")\n",
    "from pydantic import BaseModel\n",
    "\n",
    "###################################################\n",
    "## Bakehouse Sales GenieAgent \n",
    "###################################################\n",
    "\n",
    "BAKEHOUSE_GENIE_SPACE_ID = \"***\"\n",
    "bakehouse_genie_agent_description = \"\"\"このスペースは、DatabricksのAI駆動型データ分析ツールであるGenieを使用して、\n",
    "データベース内の情報を分析するためのものです。ユーザーは、SQLクエリを実行してデータを取得し、分析を行うことができます。\n",
    "提供されているテーブルには、ベーカリーフランチャイズビジネスのシミュレーションデータが含まれており、\n",
    "販売トランザクション、顧客情報、フランチャイズ情報、サプライヤー情報、メディアレビューなどが含まれています。\"\"\"\n",
    "\n",
    "def bakehouse_genie_node(state, config: RunnableConfig):\n",
    "    \"\"\"Bakehouse Sales Genie Agent node\"\"\"\n",
    "    genie_agent = GenieAgent(\n",
    "        genie_space_id=BAKEHOUSE_GENIE_SPACE_ID,\n",
    "        genie_agent_name=\"Bakehouse_Genie\",\n",
    "        description=bakehouse_genie_agent_description,\n",
    "        client=WorkspaceClient(\n",
    "            host=config['configurable'].get(\"host\"),\n",
    "            token=config['configurable'].get(\"token\"),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    result = genie_agent.invoke(state)\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": result[\"messages\"][-1].content,\n",
    "                \"name\": \"Bakehouse_Genie\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "###################################################\n",
    "## Weather Metrics GenieAgent \n",
    "###################################################\n",
    "\n",
    "WEATHER_GENIE_SPACE_ID = \"***\"\n",
    "weather_genie_agent_description = \"\"\"このスペースには、AccuWeatherの気象データを含む2つのテーブルが含まれています。\n",
    "各テーブルは、トップ50のグローバル都市の1ヶ月分の予測および歴史的気象データを提供します。\n",
    "データは、温度、湿度、降水量、風速などの気象パラメータを含み、メートル法単位で表されています。\"\"\"\n",
    "\n",
    "def weather_genie_node(state, config: RunnableConfig):\n",
    "    \"\"\"Weather Metrics Genie Agent node\"\"\"\n",
    "    genie_agent = GenieAgent(\n",
    "        genie_space_id=WEATHER_GENIE_SPACE_ID,\n",
    "        genie_agent_name=\"Weather_Genie\",\n",
    "        description=weather_genie_agent_description,\n",
    "        client=WorkspaceClient(\n",
    "            host=config['configurable'].get(\"host\"),\n",
    "            token=config['configurable'].get(\"token\"),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    result = genie_agent.invoke(state)\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": result[\"messages\"][-1].content,\n",
    "                \"name\": \"Weather_Genie\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "#############################\n",
    "# Supervisor Agent の定義\n",
    "#############################\n",
    "\n",
    "# LLMエンドポイント設定\n",
    "LLM_ENDPOINT_NAME = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n",
    "\n",
    "# 最大イテレーション数（無限ループ防止）\n",
    "MAX_ITERATIONS = 3\n",
    "\n",
    "# Worker descriptions for routing\n",
    "worker_descriptions = {\n",
    "    \"Bakehouse_Genie\": bakehouse_genie_agent_description,\n",
    "    \"Weather_Genie\": weather_genie_agent_description,\n",
    "}\n",
    "\n",
    "formatted_descriptions = \"\\n\".join(\n",
    "    f\"- {name}: {desc}\" for name, desc in worker_descriptions.items()\n",
    ")\n",
    "\n",
    "system_prompt = f\"\"\"あなたは質問を適切なワーカーエージェントにルーティングするスーパーバイザーです。\n",
    "以下のワーカーから選択するか、回答が提供された場合は会話を終了してください。\n",
    "\n",
    "{formatted_descriptions}\n",
    "\n",
    "質問の内容に基づいて、最も適切なワーカーを選択してください：\n",
    "- ベーカリー、フランチャイズ、売上、顧客、商品に関する質問 → Bakehouse_Genie\n",
    "- 天気、気象、温度、湿度、降水量に関する質問 → Weather_Genie\n",
    "- 回答が得られた場合 → FINISH\n",
    "\"\"\"\n",
    "\n",
    "options = [\"FINISH\"] + list(worker_descriptions.keys())\n",
    "FINISH = {\"next_node\": \"FINISH\"}\n",
    "\n",
    "def supervisor_agent(state):\n",
    "    \"\"\"Supervisor agent that routes to appropriate worker\"\"\"\n",
    "    count = state.get(\"iteration_count\", 0) + 1\n",
    "    if count > MAX_ITERATIONS:\n",
    "        return FINISH\n",
    "    \n",
    "    class NextNode(BaseModel):\n",
    "        next_node: Literal[tuple(options)]\n",
    "        reasoning: str = \"\"\n",
    "\n",
    "    preprocessor = RunnableLambda(\n",
    "        lambda state: [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "    )\n",
    "    supervisor_chain = preprocessor | llm.with_structured_output(NextNode)\n",
    "    \n",
    "    decision = supervisor_chain.invoke(state)\n",
    "    next_node = decision.next_node\n",
    "    \n",
    "    # 同じノードに2回連続でルーティングされた場合は終了\n",
    "    if state.get(\"next_node\") == next_node:\n",
    "        return FINISH\n",
    "    \n",
    "    return {\n",
    "        \"iteration_count\": count,\n",
    "        \"next_node\": next_node\n",
    "    }\n",
    "\n",
    "def final_answer(state):\n",
    "    \"\"\"最終回答を生成\"\"\"\n",
    "    prompt = \"\"\"以前のメッセージ内容を使用して、ユーザーの質問に対する最終的な回答を提供してください。\n",
    "アシスタントメッセージから得られた情報を整理して、わかりやすく回答してください。\"\"\"\n",
    "    \n",
    "    preprocessor = RunnableLambda(\n",
    "        lambda state: state[\"messages\"] + [{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    final_answer_chain = preprocessor | llm\n",
    "    \n",
    "    return {\"messages\": [final_answer_chain.invoke(state)]}\n",
    "\n",
    "class AgentState(ChatAgentState):\n",
    "    \"\"\"拡張されたエージェント状態\"\"\"\n",
    "    next_node: str\n",
    "    iteration_count: int\n",
    "\n",
    "# ワークフローの構築\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# ノードの追加\n",
    "workflow.add_node(\"Bakehouse_Genie\", bakehouse_genie_node)\n",
    "workflow.add_node(\"Weather_Genie\", weather_genie_node)\n",
    "workflow.add_node(\"supervisor\", supervisor_agent)\n",
    "workflow.add_node(\"final_answer\", final_answer)\n",
    "\n",
    "# エントリーポイントの設定\n",
    "workflow.set_entry_point(\"supervisor\")\n",
    "\n",
    "# ワーカーからsupervisorへのエッジ\n",
    "for worker in worker_descriptions.keys():\n",
    "    workflow.add_edge(worker, \"supervisor\")\n",
    "\n",
    "# supervisorからの条件付きエッジ\n",
    "workflow.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    lambda x: x[\"next_node\"],\n",
    "    {**{k: k for k in worker_descriptions.keys()}, \"FINISH\": \"final_answer\"},\n",
    ")\n",
    "\n",
    "# 最終回答からENDへ\n",
    "workflow.add_edge(\"final_answer\", END)\n",
    "\n",
    "# グラフのコンパイル\n",
    "multi_agent = workflow.compile()\n",
    "\n",
    "###################################\n",
    "# ChatAgentラッパーの実装\n",
    "###################################\n",
    "\n",
    "class LangGraphChatAgent(ChatAgent):\n",
    "    \"\"\"LangGraphをMLflow ChatAgentとしてラップ\"\"\"\n",
    "    \n",
    "    def __init__(self, agent: CompiledStateGraph):\n",
    "        self.agent = agent\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        \"\"\"同期的な予測\"\"\"\n",
    "        request = {\n",
    "            \"messages\": [m.model_dump_compat(exclude_none=True) for m in messages]\n",
    "        }\n",
    "\n",
    "        messages = []\n",
    "        for event in self.agent.stream(request, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                messages.extend(\n",
    "                    ChatAgentMessage(**msg) for msg in node_data.get(\"messages\", [])\n",
    "                )\n",
    "        return ChatAgentResponse(messages=messages)\n",
    "\n",
    "    def predict_stream(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> Generator[ChatAgentChunk, None, None]:\n",
    "        \"\"\"ストリーミング予測\"\"\"\n",
    "        request = {\n",
    "            \"messages\": [m.model_dump_compat(exclude_none=True) for m in messages]\n",
    "        }\n",
    "        for event in self.agent.stream(request, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                yield from (\n",
    "                    ChatAgentChunk(**{\"delta\": msg})\n",
    "                    for msg in node_data.get(\"messages\", [])\n",
    "                )\n",
    "\n",
    "# エージェントオブジェクトの作成とMLflowへの設定\n",
    "mlflow.langchain.autolog()\n",
    "AGENT = LangGraphChatAgent(multi_agent)\n",
    "mlflow.models.set_model(AGENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b1fb6b2-3926-4bae-a8ac-e66be84682a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## エージェントのテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "998eaa6f-65a6-4d7a-8b1c-96a807b9b3ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ad8e713-d6ed-4530-b0ae-eb50c0f482ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from agent import multi_agent\n",
    "\n",
    "# エージェントグラフの構造を表示\n",
    "multi_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08098a16-7ff2-42fe-9c7f-18ab005faee9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from agent import AGENT\n",
    "\n",
    "# Bakehouse関連の質問\n",
    "response = AGENT.predict(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"売上トップ3のフランチャイズ店舗はどこですか？\"}]}\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "for msg in response.messages:\n",
    "    print(f\"\\n[{msg.role}]: {msg.content[:500]}...\" if len(msg.content) > 500 else f\"\\n[{msg.role}]: {msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91e3e7d9-9e6b-4396-abb1-e757955a690d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Weather関連の質問\n",
    "response = AGENT.predict(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"東京の2024年7月の平均気温は何度ですか？\"}]}\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "for msg in response.messages:\n",
    "    print(f\"\\n[{msg.role}]: {msg.content[:500]}...\" if len(msg.content) > 500 else f\"\\n[{msg.role}]: {msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9a5f66e-64f4-4283-aa96-1344bf488517",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## まとめ\n",
    "\n",
    "このノートブックでは、以下を実装しました：\n",
    "\n",
    "1. **databricks_langchain.genie.GenieAgent** を使用した2つのGenie Agentの作成\n",
    "2. **Supervisor Agent** による自動ルーティング機能\n",
    "3. **langgraph StateGraph** によるマルチエージェントシステムの構築\n",
    "\n",
    "### 嬉しさ\n",
    "- Genie Conversation API は似た関数・クラスが多く実装がやや複雑だが、GenieAgent という高水準 API がそれらをカプセル化してくれるため、開発が楽になる。\n",
    "- ツール呼び出し（Tool Calling）をサポートしない基盤モデルも利用できる。"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_Agent_with_Genie_Agent",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
